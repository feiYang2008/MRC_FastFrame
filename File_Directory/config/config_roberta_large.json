{
  "global": {
    "app_name": "validate_stage2_roberta_large_lstm",
    "use_parallel": false,
    "num_of_device": 1,
    "batch_size": 4,
    "pretrained_model_type": "roberta",
    "load_model_path": "File_Directory/models/train_roberta_large_lstm_step12000_2020-03-26_16-31-39"
  },
  "dataset": {
    "vocab_name": "vocab_roberta",
    "max_seq_length": 512,
    "shuffle": true
  },
  "build": {
    "hidden_size": 1024,
    "num_hidden_layers": 24,
    "num_attention_heads": 16,
    "hidden_act": "gelu",
    "vocab_size": 21128,
    "mrc_layer": "lstm",
    "freeze_pretrained_model": false

  },
  "train": {
    "max_epoch": 2,
    "validate_frequency_step": 2000,
    "snapshot_frequency": 2000,
    "base_learning_rate": 1e-6,
    "learning_rate_strategy": "noam_worm_up_and_decay",
    "warm_up_step": 2200,
    "optimizer": "adam",
    "continue_train": false,
    "regularization": "L2",
    "regularization_coeff": 0.1,
    "gradient_clip": true,
    "clip_norm": 0.1,
    "read_checkpoint": false
  },
  "predict": {

  }
}
